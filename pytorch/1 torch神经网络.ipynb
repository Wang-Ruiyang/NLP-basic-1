{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.0191,  0.0933,  0.1500,  0.0929,  0.1964, -0.0390,  0.1997, -0.1673,\n          0.0278, -0.0033],\n        [-0.0475,  0.0938,  0.1908,  0.0760, -0.0488,  0.1465,  0.1322, -0.1653,\n          0.0159, -0.1065]], grad_fn=<AddmmBackward>)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.Linear(20,256), nn.ReLU(), nn.Linear(256,10))\n",
    "\n",
    "# X = torch.rand(2,20) 这行代码会生成一个大小为 2x20 的张量，其中包含了 40 个在区间 [0, 1) 内均匀分布的随机数。\n",
    "X = torch.rand(2,20)     # 输入\n",
    "\n",
    "net(X)   # 输出"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.1107, -0.0152, -0.0482,  0.0857, -0.0134, -0.1858,  0.1200, -0.0125,\n         -0.2319,  0.2418],\n        [ 0.1450, -0.0364,  0.0737,  0.0632, -0.0144, -0.2498,  0.1886,  0.0132,\n         -0.2994,  0.2674]], grad_fn=<AddmmBackward>)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20,256)\n",
    "        self.out = nn.Linear(256,10)\n",
    "    def forward(self, X):\n",
    "        return self.out(F.relu((self.hidden(X))))      # 将三个层链接起来\n",
    "\n",
    "net = MyMLP()\n",
    "X = torch.rand(2,20)\n",
    "net(X)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=20, out_features=256, bias=True)\n",
      "ReLU()\n",
      "Linear(in_features=256, out_features=10, bias=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[ 0.1465, -0.2628, -0.2069, -0.0105,  0.1629,  0.0803,  0.2052, -0.0417,\n         -0.0517,  0.1107],\n        [ 0.1986, -0.1893, -0.1800,  0.1601,  0.2471,  0.1711,  0.1583, -0.0239,\n         -0.0810,  0.0832]], grad_fn=<AddmmBackward>)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self,*args):\n",
    "        super().__init__()\n",
    "        for block in args:\n",
    "            print(block)\n",
    "            self._modules[block] = block\n",
    "    def forward(self,X):\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X\n",
    "\n",
    "net = MySequential(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,10))\n",
    "net(X)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.0536,  0.0049, -0.0555, -0.0426,  0.0314, -0.0742, -0.1588, -0.0821,\n          0.0488, -0.1305],\n        [ 0.0572,  0.0065, -0.0543, -0.0418,  0.0279, -0.0718, -0.1583, -0.0794,\n          0.0450, -0.1323]], grad_fn=<AddmmBackward>)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20,64),nn.ReLU(),\n",
    "                                 nn.Linear(64,32),nn.ReLU())\n",
    "        self.linear = nn.Linear(32,16)\n",
    "    def forward(self,X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(),nn.Linear(16,20),MyMLP())\n",
    "X = torch.rand(2,20)\n",
    "chimera(X)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.1880],\n        [0.1960]], grad_fn=<AddmmBackward>)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.Linear(4,8), nn.ReLU(), nn.Linear(8,1))\n",
    "X = torch.rand(size=(2,4))\n",
    "net(X)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[ 0.1474, -0.0005,  0.3029,  0.3496, -0.2981, -0.2374, -0.1073,  0.3436]])), ('bias', tensor([0.2570]))])\n",
      "\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "\n",
      "Parameter containing:\n",
      "tensor([0.2570], requires_grad=True)\n",
      "\n",
      "tensor([0.2570])\n",
      "\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(net[2].state_dict())\n",
    "print()\n",
    "print(type(net[2].bias))\n",
    "print()\n",
    "print(net[2].bias)\n",
    "print()\n",
    "print(net[2].bias.data)\n",
    "print()\n",
    "print(net[2].weight.grad==None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n",
      "\n",
      "tensor([0.2570])\n"
     ]
    }
   ],
   "source": [
    "print(*[(name,param.shape)for name,param in net[0].named_parameters()])\n",
    "print()\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()])\n",
    "print()\n",
    "print(net.state_dict()['2.bias'].data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.0334],\n        [0.0334]], grad_fn=<AddmmBackward>)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4,8), nn.ReLU(), nn.Linear(8,4), nn.ReLU())\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        net.add_module(f'block {i}',block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(block2(), nn.Linear(4,1))\n",
    "rgnet(X)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block 0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rgnet)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([-0.0038, -0.0017, -0.0045,  0.0039]), tensor(0.))"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_normal(m):    # 传入的是一个module\n",
    "    if type(m) == nn.Linear:     # 如果是线性层\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)     # _表示无需返回值，直接原地替换。这里将weight变成均值为0，标准差为0.01\n",
    "        nn.init.zeros_(m.bias)    # 将bias偏移替换为0\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4,8), nn.ReLU(), nn.Linear(8,1))\n",
    "X = torch.rand(size=(2,4))\n",
    "net(X)\n",
    "\n",
    "net.apply(init_normal)      # 将网络遍历一遍，用init_normal进行更新\n",
    "net[0].weight.data[0],net[0].bias.data[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([1., 1., 1., 1.]), tensor(0.))"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_constant(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4,8), nn.ReLU(), nn.Linear(8,1))\n",
    "X = torch.rand(size=(2,4))\n",
    "net(X)\n",
    "\n",
    "net.apply(init_constant)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.5082, -0.0528, -0.1069,  0.1827])\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "def xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)      # 对weight做xavier均值化\n",
    "\n",
    "def init_42(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 42)      # 全部赋值42\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4,8), nn.ReLU(), nn.Linear(8,1))\n",
    "X = torch.rand(size=(2,4))\n",
    "net(X)\n",
    "\n",
    "net[0].apply(xavier)\n",
    "net[2].apply(init_42)\n",
    "\n",
    "print(net[0].weight.data[0])\n",
    "print(net[2].weight.data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init weight torch.Size([8, 4])\n",
      "Init weight torch.Size([1, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[8.3302, 0.0000, 0.0000, -0.0000],\n        [9.1558, 9.6040, -0.0000, 0.0000]], grad_fn=<SliceBackward>)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        print(\n",
    "            \"Init\",\n",
    "            *[(name,param.shape) for name, param in m.named_parameters()][0]\n",
    "        )\n",
    "        nn.init.uniform_(m.weight, -10, 10)      # (-10,10)之间的均匀分布填充\n",
    "        m.weight.data *= m.weight.data.abs()>=5     # 表示如果 m.weight.data.abs()>=5成立，相当于1；不成立相当于0\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4,8), nn.ReLU(), nn.Linear(8,1))\n",
    "X = torch.rand(size=(2,4))\n",
    "net(X)\n",
    "\n",
    "net.apply(my_init)\n",
    "net[0].weight[:2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([42.,  1.,  1.,  1.])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data[:] += 1     # 所有值+1\n",
    "net[0].weight.data[0, 0] = 42     # 第一行的第一个元素为42\n",
    "net[0].weight.data[0]      # 第一行元素"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "shared = nn.Linear(8,8)\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(4,8),nn.ReLU(),shared,nn.ReLU(),shared,nn.ReLU(),nn.Linear(8,1)\n",
    ")\n",
    "net(X)\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "\n",
    "net[2].weight.data[0,0] = 100\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([-2., -1.,  0.,  1.,  2.])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CenteredLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, X):\n",
    "        return X - X.mean()\n",
    "\n",
    "layer = CenteredLayer()\n",
    "layer(torch.FloatTensor([1,2,3,4,5]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(7.4506e-09, grad_fn=<MeanBackward0>)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.Linear(8,128), CenteredLayer())\n",
    "\n",
    "Y = net(torch.rand(4,8))\n",
    "Y.mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "Parameter containing:\ntensor([[-1.4344,  0.5694, -0.6562],\n        [ 0.8137,  0.1474, -1.5661],\n        [-0.4021,  0.6279,  0.7612],\n        [-0.3380, -1.6948,  0.6764],\n        [-0.6898,  0.9873,  1.0886]], requires_grad=True)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_units, units):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_units,units))      # 均值为0，方差为1的正态分布，模型形状为 in_units*units\n",
    "        self.bias = nn.Parameter(torch.zeros(units))\n",
    "    def forward(self,X):\n",
    "        linear = torch.matmul(X, self.weight.data) + self.bias.data\n",
    "        return F.relu(linear)\n",
    "\n",
    "dense = MyLinear(5,3)\n",
    "dense.weight"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.2325, 0.0000, 0.0000],\n        [0.0000, 0.5239, 0.9386]])"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense(torch.rand(2,5))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.],\n        [0.]])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(MyLinear(64,8), MyLinear(8,1))\n",
    "net(torch.rand(2,64))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0, 1, 2, 3])"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4)\n",
    "torch.save(x, 'x-file')      # 保存数据到文件\n",
    "\n",
    "x2 = torch.load(\"x-file\")    # 从文件读取数据\n",
    "x2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.zeros(4)\n",
    "torch.save([x,y], 'x-files')\n",
    "x2, y2 = torch.load('x-files')\n",
    "(x2, y2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "{'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict = {'x': x, 'y': y}\n",
    "torch.save(mydict, 'mydict')\n",
    "mydict2 = torch.load('mydict')\n",
    "mydict2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('hidden.weight', tensor([[ 0.0971, -0.1579,  0.0044,  ..., -0.0998,  0.0737, -0.1686],\n",
      "        [ 0.0530,  0.0414,  0.1216,  ...,  0.1181, -0.0419,  0.2201],\n",
      "        [-0.1028, -0.1758, -0.1662,  ..., -0.0928,  0.1294,  0.0587],\n",
      "        ...,\n",
      "        [-0.1926, -0.1774,  0.2134,  ..., -0.2065, -0.1961, -0.0740],\n",
      "        [ 0.1167,  0.1235,  0.0281,  ..., -0.2233, -0.1973,  0.2134],\n",
      "        [ 0.1455,  0.0599,  0.1002,  ...,  0.0954, -0.1207, -0.1105]])), ('hidden.bias', tensor([-0.0302, -0.0348,  0.0281,  0.0997, -0.0086,  0.1300,  0.0476, -0.2219,\n",
      "         0.1233,  0.0026, -0.0673,  0.0856, -0.1732,  0.1215, -0.1404, -0.1227,\n",
      "         0.2165,  0.1321, -0.0280,  0.0206, -0.0785, -0.2105, -0.0730,  0.1728,\n",
      "         0.0761,  0.1807,  0.1850,  0.1304,  0.1791, -0.1322, -0.1459, -0.2088,\n",
      "         0.1379,  0.0745, -0.1551,  0.1028, -0.0318, -0.0190,  0.0452, -0.1210,\n",
      "        -0.1974,  0.0848, -0.1211, -0.0029, -0.0758, -0.1973, -0.1604,  0.1820,\n",
      "         0.1789,  0.0604,  0.2105,  0.1426, -0.2211,  0.0429, -0.0488, -0.0870,\n",
      "         0.0529, -0.1741, -0.1400,  0.0548, -0.1349,  0.0981,  0.2169, -0.0210,\n",
      "        -0.1844,  0.0077,  0.0834,  0.2054, -0.2228, -0.0648,  0.0173,  0.0247,\n",
      "        -0.1176,  0.1128,  0.1654, -0.0504,  0.1098,  0.1063, -0.0364, -0.1849,\n",
      "        -0.1402,  0.1194, -0.0215, -0.0185, -0.2194,  0.0617, -0.1944,  0.1957,\n",
      "        -0.0532,  0.1535,  0.2023, -0.1782,  0.2102, -0.1350,  0.0877, -0.0950,\n",
      "        -0.1563, -0.0329,  0.0251,  0.0778,  0.1942, -0.1021,  0.0153,  0.1390,\n",
      "         0.1445,  0.0628,  0.0339,  0.2018, -0.0993,  0.0693, -0.2129,  0.0332,\n",
      "        -0.0043,  0.0372, -0.0691,  0.2017, -0.0454, -0.0628, -0.1467,  0.0851,\n",
      "         0.1321, -0.2065, -0.0110,  0.0214, -0.0256,  0.1904,  0.0079, -0.0249,\n",
      "        -0.1835, -0.1258, -0.0364,  0.0382,  0.0364,  0.0556,  0.0968, -0.0379,\n",
      "        -0.0573, -0.0327,  0.2173, -0.1410,  0.0369, -0.0393,  0.0957, -0.0846,\n",
      "         0.1420, -0.0529,  0.0196, -0.1414,  0.0247,  0.0764, -0.0029,  0.1371,\n",
      "         0.0078,  0.0094,  0.1526, -0.0658, -0.1047, -0.0852,  0.1926,  0.1918,\n",
      "         0.1632, -0.0534,  0.0203,  0.1192,  0.0354, -0.1002,  0.2012, -0.1022,\n",
      "         0.1445,  0.1265,  0.1041,  0.0924, -0.0209,  0.0056, -0.1787,  0.0652,\n",
      "        -0.1389, -0.0571, -0.1906, -0.2193, -0.0129,  0.0469, -0.0718,  0.2137,\n",
      "        -0.0676,  0.2137, -0.0784, -0.0154,  0.0074, -0.0139,  0.2043,  0.1941,\n",
      "        -0.0824,  0.0544, -0.2138, -0.0478, -0.1863, -0.2089,  0.1727,  0.0725,\n",
      "         0.0170,  0.2099, -0.2147, -0.2158, -0.1833, -0.1895, -0.1178, -0.0157,\n",
      "        -0.1715, -0.0400,  0.0310, -0.2036, -0.1314,  0.1275,  0.0224, -0.1556,\n",
      "        -0.0607, -0.0977,  0.0341, -0.0254, -0.1512,  0.1797, -0.0274, -0.0814,\n",
      "        -0.1371,  0.0022, -0.1917, -0.1770,  0.0264,  0.0580,  0.0983,  0.0687,\n",
      "         0.0243,  0.1223,  0.1731, -0.0675,  0.0962,  0.1003, -0.1203,  0.2133,\n",
      "        -0.2032,  0.0220, -0.0204,  0.0468, -0.1539,  0.0375,  0.0204, -0.0473,\n",
      "        -0.0527,  0.0048, -0.1396, -0.0565,  0.0003,  0.0638, -0.0141, -0.1569])), ('output.weight', tensor([[-0.0125,  0.0369, -0.0440,  ...,  0.0466, -0.0250,  0.0114],\n",
      "        [ 0.0407, -0.0135,  0.0389,  ...,  0.0151,  0.0203,  0.0166],\n",
      "        [ 0.0319, -0.0301,  0.0393,  ...,  0.0548, -0.0589, -0.0623],\n",
      "        ...,\n",
      "        [-0.0496, -0.0027, -0.0179,  ...,  0.0543,  0.0416, -0.0003],\n",
      "        [ 0.0338, -0.0056, -0.0475,  ...,  0.0057, -0.0026,  0.0187],\n",
      "        [ 0.0377,  0.0057, -0.0228,  ..., -0.0383, -0.0076,  0.0280]])), ('output.bias', tensor([ 0.0501,  0.0566,  0.0253, -0.0438, -0.0322, -0.0575, -0.0386, -0.0467,\n",
      "        -0.0455, -0.0255]))])\n"
     ]
    }
   ],
   "source": [
    "class MyMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20,256)\n",
    "        self.output = nn.Linear(256,10)\n",
    "    def forward(self, X):\n",
    "        return self.output(F.relu((self.hidden(X))))      # 将三个层链接起来\n",
    "\n",
    "net = MyMLP()\n",
    "X = torch.randn(size=(2,20))\n",
    "Y = net(X)\n",
    "print(net.state_dict())\n",
    "torch.save(net.state_dict(),'mlp.params')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyMLP(\n",
      "  (hidden): Linear(in_features=20, out_features=256, bias=True)\n",
      "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "clone = MyMLP()     # 先声明一个MyMLP，此时网络已经初始化\n",
    "clone.load_state_dict(torch.load(\"mlp.params\"))    # 更新权重\n",
    "print(clone.eval())\n",
    "\n",
    "Y_clone = clone(X)\n",
    "print(Y_clone == Y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
